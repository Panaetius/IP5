Hyper Parameters:
NUM_EPOCHS_PER_DECAY = 10.0
LEARNING_RATE_DECAY_FACTOR = 0.5
INITIAL_LEARNING_RATE = 0.0001
WEIGHT_DECAY = INITIAL_LEARNING_RATE * 0.01
batch = 32

-->92.7%, valid: 87.1%, 7:20



Hyper Parameters (1):
NUM_EPOCHS_PER_DECAY = 30.0
LEARNING_RATE_DECAY_FACTOR = 0.5
INITIAL_LEARNING_RATE = 0.0002
WEIGHT_DECAY = INITIAL_LEARNING_RATE * 0.01
batch = 32

--> 87.1%, valid: 81.5%, 9:14
	2016-10-13 16:52:25.866273: precision @ 1 = 0.815
	precisions: [ 0.94991055  0.93846154  1.          0.99570201  1.          0.85022523
	  0.4143469   0.36503856  0.84304933  0.95483871]
	recalls: [ 0.94652406  0.953125    0.99514563  0.99570201  1.          0.8014862
	  0.83585313  0.35544431  0.61237785  0.96802326]
	f1: [ 0.94821429  0.94573643  0.99756691  0.99570201  1.          0.82513661
	  0.55404438  0.36017755  0.70943396  0.96138578]



Hyper Parameters (2):
NUM_EPOCHS_PER_DECAY = 30.0
LEARNING_RATE_DECAY_FACTOR = 0.5
INITIAL_LEARNING_RATE = 0.00007
WEIGHT_DECAY = 0.0000007
batch = 32

--> 94.1%, 4:59
	2016-10-13 21:49:58.973936: precision @ 1 = 0.898
	precisions: [ 0.97674419  0.93230769  1.          1.          1.          0.93806306
	  0.7642015   0.75578406  0.767713    0.97279885]
	recalls: [ 0.975       0.95283019  1.          0.98171589  1.          0.85086823
	  0.92959583  0.68933177  0.76021314  0.98051948]
	f1: [ 0.97587131  0.94245723  1.          0.9907736   1.          0.89234065
	  0.83882353  0.72103004  0.76394467  0.97664391]


Hyper Parameters (3):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 30.0     
LEARNING_RATE_DECAY_FACTOR = 0.5 
INITIAL_LEARNING_RATE = 0.00005  
WEIGHT_DECAY = 0.00003
batch = 32

--> 96.5%, 8:15
	2016-10-14 07:10:17.269606: precision @ 1 = 0.909
	precisions: [ 0.92665474  0.93538462  0.99348534  0.99713467  1.          0.89076577
	  0.83190578  0.86503856  0.79103139  0.96487455]
	recalls: [ 0.95925926  0.91291291  0.99025974  0.97206704  1.          0.86259542
	  0.85856354  0.76477273  0.87848606  0.9704398 ]
	f1: [ 0.94267516  0.92401216  0.99186992  0.9844413   1.          0.87645429
	  0.84502447  0.81182147  0.83246815  0.96764917]



Hyper Parameters (4):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 30.0     
LEARNING_RATE_DECAY_FACTOR = 0.5 
INITIAL_LEARNING_RATE = 0.00005  
WEIGHT_DECAY = 0.00005
batch = 32

--> 96.0%, 9:15
	2016-10-14 17:21:55.653265: precision @ 1 = 0.928
	precisions: [ 0.96064401  0.91846154  1.          1.          1.          0.94713161
	  0.83315508  0.89974293  0.83034111  0.97706093]
	recalls: [ 0.96236559  0.96290323  0.99837398  0.98448519  1.          0.92833517
	  0.94195889  0.75512406  0.85648148  0.9812815 ]
	f1: [ 0.96150403  0.94015748  0.99918633  0.99218195  1.          0.9376392
	  0.88422247  0.82111437  0.84320875  0.97916667]


Hyper Parameters (5):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 30.0     
LEARNING_RATE_DECAY_FACTOR = 0.25 
INITIAL_LEARNING_RATE = 0.00005  
WEIGHT_DECAY = 0.0005
batch = 20

--> 97.4%, 11:52
	2016-10-15 11:07:06.526535: precision @ 1 = 0.900
	precisions: [ 0.96243292  0.93404908  1.          1.          1.          0.91788526
	  0.82976445  0.81491003  0.72172352  0.96484935]
	recalls: [ 0.97640653  0.96666667  0.98397436  0.99147727  1.          0.90365449
	  0.85825028  0.67232238  0.80239521  0.9803207 ]
	f1: [ 0.96936937  0.950078    0.99192246  0.9957204   1.          0.91071429
	  0.84376701  0.73678094  0.75992439  0.9725235 ]

Hyper Parameters (6) (with ELU not RELU):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 10.0     
LEARNING_RATE_DECAY_FACTOR = 0.5 
INITIAL_LEARNING_RATE = 0.0001  
WEIGHT_DECAY = 0.005 (x15 layer1, x1.3 layer2, x0.1 layer17)
batch = 20

--> 95.6%, 14:15, 90.02% test

Hyper Parameters (7) (with ELU not RELU):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 12.5     
LEARNING_RATE_DECAY_FACTOR = 0.5 
INITIAL_LEARNING_RATE = 0.00007  
WEIGHT_DECAY = 0.002 (x10 layer1, x1.3 layer2, x0.1 layer17)
DROPOUT (FC Layers) = 75%
batch = 22

--> 98%, 17:40
	2016-10-17 14:50:37.237786: precision @ 1 = 0.922
	precisions: [ 1.          0.99435028  0.98115942  1.          1.          0.84466019
	  0.87061995  0.82139037  0.82146161  0.99183197]
	recalls: [ 1.          1.          1.          0.99622642  1.          0.91901408
	  0.80548628  0.74418605  0.88446215  0.98265896]
	f1: [ 1.          0.99716714  0.99049012  0.99810964  1.          0.88026981
	  0.83678756  0.7808846   0.85179856  0.98722416]


Hyper Parameters (8) (with ELU not RELU):
MOVING_AVERAGE_DECAY = 0.9999   
NUM_EPOCHS_PER_DECAY = 12.5     
LEARNING_RATE_DECAY_FACTOR = 0.5 
INITIAL_LEARNING_RATE = 0.00007  
WEIGHT_DECAY = 0.0015 (x10 layer1, x1.3 layer2, x0.1 layer17)
DROPOUT (FC Layers) = 50%
batch = 22

--> 98.6%, 1d 1:55
	2016-10-18 20:14:51.750171: precision @ 1 = 0.935, @ 2 = 0.977
	precisions: [ 1.          0.99576271  0.99565217  0.99337121  1.          0.86731392
	  0.89636608  0.85775401  0.85555556  0.97780374]
	recalls: [ 0.99837134  0.98601399  1.          1.          1.          0.91990847
 	 0.91232877  0.75517891  0.89361702  0.99288256]
	f1: [ 0.999185    0.99086437  0.99782135  0.99667458  1.          0.89283731
 	 0.90427699  0.80320481  0.87417219  0.98528546]
